import os
from typing import Annotated, TypedDict, List
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage, ToolMessage
from langchain_core.tools import StructuredTool

from app.utils.logger import get_logger
from app.services.tool_registry import ToolRegistry
from app.services.mcp_bridge import OpenAPIMCPBridge

load_dotenv()
logger = get_logger("Agent_Brain")

# global tools
registry = ToolRegistry()


class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    available_tools: List[StructuredTool]


def tool_retriever_node(state: AgentState):
    """
    Analyzes the last user message and fetches relevant tools from the Registry.
    """
    last_message = state["messages"][-1]
    query = last_message.content

    logger.info(f"Retrieving tools for query: '{query}'")
    tools = registry.search_tools(query, k=5)

    # Store these tools in the state so the next node can use them
    return {"available_tools": tools}


def reasoner_node(state: AgentState):
    """
    Binds the retrieved tools to the LLM and asks for a decision.
    """
    system_prompt = SystemMessage(content="""
    You are an autonomous AI Integration Agent.
    Your goal is to satisfy the user's request using the available tools.
    
    RULES:
    1. If a tool seems relevant, USE IT immediately. Do not ask for permission for read-only operations (GET).
    2. If the user asks for a specific field (like 'status') and you have a tool that returns the full object (like 'getPetById'), CALL THE TOOL first, then extract the field from the result.
    3. Only ask the user for clarification if you are missing required arguments (like an ID).
    """)

    tools = state["available_tools"]
    messages = state["messages"]
    full_history = [system_prompt] + messages

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        api_key=os.getenv("GOOGLE_API_KEY")
    )

    if tools:
        llm_with_tools = llm.bind_tools(tools)
        response = llm_with_tools.invoke(full_history)
    else:
        response = llm.invoke(full_history)

    return {"messages": [response]}


def tool_executor_node(state: AgentState):
    """
    Executes the tool calls generated by the LLM.
    """
    last_message = state["messages"][-1]
    tools = state["available_tools"]
    tool_map = {t.name: t for t in tools}

    outputs = []

    if hasattr(last_message, "tool_calls"):
        for call in last_message.tool_calls:
            tool_name = call["name"]
            args = call["args"]

            logger.info(f"Executing Tool: {tool_name} with args: {args}")

            if tool_name in tool_map:
                try:
                    result = tool_map[tool_name].invoke(args)
                    output_content = str(result)
                except Exception as e:
                    output_content = f"Error: {str(e)}"
            else:
                output_content = "Error: Tool not found in available tools."

            # this ToolMessage output is feeded back to llm
            outputs.append(ToolMessage(
                content=output_content,
                tool_call_id=call["id"],
                name=tool_name
            ))

    return {"messages": outputs}


def should_continue(state: AgentState):
    """
    Decides if we should respond to user or execute a tool.
    """
    last_message = state["messages"][-1]

    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "executor"
    return END


# building the graph
workflow = StateGraph(AgentState)

workflow.add_node("retriever", tool_retriever_node)
workflow.add_node("reasoner", reasoner_node)
workflow.add_node("executor", tool_executor_node)

workflow.add_edge(START, "retriever")
workflow.add_edge("retriever", "reasoner")

# conditional to decide between replying or executing
workflow.add_conditional_edges(
    "reasoner",
    should_continue,
    {
        "executor": "executor",
        END: END
    }
)

workflow.add_edge("executor", "reasoner")

agent_app = workflow.compile()
